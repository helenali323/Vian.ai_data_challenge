---
title: "Nov 29 Notes"
author: "Hongdou Li"
date: "11/29/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Multivariate Time Series

Unitl now, we've only considered **univariate** time series, i.e, we have temporal observations on one variable which use to forecast that variable. 

Now, imagine you have data on other variables collected at the same frequency and the same duration as your response series. Furthermore, if these extra variables are highly correlated with the response, we may exploit to incorporate their observation into a **multivariate** time series model. We hope that such a model provides more accurate forecasts than some other univariate model.

Depending on how we want to treat this external information determines which modeling approach to take: 

+ If we treat these variables as **exogenous**. i.e, they influence the response, but not the other way around, We can fit a  *SARIMAX* model to account for this type of relationship

EX: Daily BART ridership and daily weather

+ If we treat these variables as **endodenous**. i.e, they influence the response and the response influences them, then we can use vactor autoregression (VAR) to simultaneously account for all of these dependencies.

EX: Daily closing price of APPLE atock and daily closing price of Amazon stock.

## SARIMAX MODEL

A SARIMAX model is thought of as a SARIMA with exponentory variables. To begin consider an ARMAX(p,q) model:

$Y_t=\phi_1Y_{t-1}+...+\phi_pY_{t-p}+\epsilon_t+\theta_1\epsilon_{t-1}+...+\theta_q\epsilon_{t-q}+\beta_1X_{1,t}+\beta_2X_{2,t}+...+\beta_rX_{r,t}$

>$=\sum_{i=1}^p\phi_iY_{t-i}+\epsilon_t+\sum_{j=1}^q\theta_j\epsilon_{t-j}+\sum_{k=1}^r\beta_kX_{k,t}$

where $\{X_{k,t}\}$ is an explantory, "exogenous", time series, k=1,2,...,r 

If $\{Y_t\}$ is not stationary, we difference it as necessary to become stationary. The same level of differencing is also automatically applied to the exogenous series. This gives rise to ARIMAX and SARIMAX models.

$\sum_{k=1}^r\beta_kX_{k,t}<=Corr(Y_t,X_{k,t+h}, h\in Z$ is called cross correlation, and can be useful in determining how the X information is used.

\* In order to forecast a SARIMAX model we need future values of the exogenous variables, or predictions of them.

The main limitation of the SARIMAX model is that it cannot account for a bi-directional relationship if one exists. In this situation a vector autoregression would be more appropriate, and may provide more accurate forecasts.

## Vector Autoregression

In this framework all variables are treated symmetrically and so we revise our notation and denote r endongenous variables as :$\{Y_{1,t}\},\{Y_{2,t}\},...\{Y_{r,t}\}$. This model consists of r equations (one for each variable) that are each autoregressions of order p.

VAR(p): 

$Y_{1,t} = c_1 \sum_{i=1}^p\phi_{11,i}Y_{1,t-i}+\sum_{i=1}^p\phi_{12,i}Y_{2,t-i}+...+\sum_{i=1}^p\phi_{1r,i}Y_{r,t-i}+\epsilon_{1,t}$

>>.

>>.

>>.

$Y_{r,t} = c_1 \sum_{i=1}^p\phi_{r1,i}Y_{1,t-i}+\sum_{i=1}^p\phi_{r2,i}Y_{1,t-i}+...+\sum_{i=1}^p\phi_{rr,i}Y_{r,t-i}+\epsilon_{r,t}$

where $\{\epsilon_{k,t}\}\sim WN(0,\sigma_k^2$

def $\overrightarrow{Y_t} =\begin{bmatrix}Y_{1,t}\\Y_{2,t}\\.\\.\\.\\Y_{r,t}\end{bmatrix}$
$\overrightarrow{c} =\begin{bmatrix}c_{1}\\c_{2}\\.\\.\\.\\c_{r}\end{bmatrix}$
$\overrightarrow{\epsilon_t} =\begin{bmatrix}\epsilon_{1,t}\\\epsilon_{2,t}\\.\\.\\.\\\epsilon_{r,t}\end{bmatrix}$

$A_i =\begin{bmatrix}\phi_{11,i}&\phi_{12,i}&...&\phi_{1r,i}\\\phi_{21,i}&\phi_{22,i}&...&\phi_{2r,i}\\.\\.\\.\\\phi_{r1,i}&\phi_{r2,i}&...&\phi_{rr,i}\end{bmatrix}$

Using this vector-matrix notation we can equivalently write a VAR(p) model more succinctly as:

$\overrightarrow{Y_t}=\overrightarrow{c}+a_1\overrightarrow{Y_{t-1}}+A_2\overrightarrow{Y_{t-2}}+...+A_p\overrightarrow{Y_{t-p}}+\overrightarrow{\epsilon_t}$

EX: VAR(1) with two variables

$\overrightarrow{Y_{1,t}} = c_1+\phi_{11,1}\overrightarrow{Y_{1,t-1}}+\phi_{12,1}\overrightarrow{Y_{2,t-1}}+\overrightarrow{\epsilon_{1,t}}$

$\overrightarrow{Y_{2,t}} = c_2+\phi_{21,1}\overrightarrow{Y_{1,t-1}}+\phi_{22,1}\overrightarrow{Y_{2,t-1}}+\overrightarrow{\epsilon_{2,t}}$

$\overrightarrow{Y_t}=\begin{bmatrix}Y_{1,t}\\Y_{2,t}\end{bmatrix}$
$\overrightarrow{c}=\begin{bmatrix}c_{1}\\c_{2}\end{bmatrix}$
$\overrightarrow{\epsilon_t}=\begin{bmatrix}\epsilon_{1,t}\\\epsilon_{2,t}\end{bmatrix}$
$A=\begin{bmatrix}\phi_{11,1}&\phi_{12,1}\\\phi_{21,1}&\phi_{22,1}\end{bmatrix}$
$\overrightarrow{Y_{t-1}}=\begin{bmatrix}Y_{1,t-1}\\Y_{2,t-1}\end{bmatrix}$

The general VAR(p) model contains $2r + pr^2$ parameters. To simplify estimation avoid overfitting we typically try to keep r and/or p small. Order selection in this is based on predictive accuracy and/or goodness-of-fit. Estimation is typically carried out with leadt squares.

\* VARMA models exist, but are not commonly used in practice

\* VARX models are commonly used when you have exofenous and endogenous variables

```{r}
library(forecast)
library(vars)
```

```{r}
data <- read.csv("consumer.csv", header=T)
head(data)
train <- data[1:252,]
test <- data[253:255,]
Bev <- ts(train$BevInd, start = c(1991, 1), frequency = 12)
Food <- ts(train$FoodInd, start = c(1991, 1), frequency = 12)
Indust <- ts(train$IndustInd, start = c(1991, 1), frequency = 12)
```

```{r}
par(mfrow=c(3,1))
plot(Bev)
plot(Food)
plot(Indust)
```

```{r}
par(mfrow=c(3,1))
ccf(Bev, Food)
ccf(Bev, Indust)
ccf(Food, Indust)
```

```{r}
# ordinary difference:
par(mfrow=c(2,1))
plot(diff(Bev))
acf(diff(Bev), lag.max = 72)
```

```{r}
# order selection:
par(mfrow=c(2,1))
acf(diff(Bev), lag.max = 72)
pacf(diff(Bev), lag.max = 72)
#p=q=1 seems fine
```

```{r}
# Fit an ARIMA(1,1,1) model
m1 <- arima(Bev, order = c(1,1,1))
m1
tsdiag(m1)
```

ACF: $H_0: \rho(0)=0$ vs. $H_a: \rho(h)\ne0$

Ljung-Box: $H_0:\rho(1)=\rho(2)=...=\rho(H)=0$ vs. $H_A:\rho(h)\ne0$ for same h=1,2....H

```{r}
# Fit an ARIMAX(1,1,1) model with covariate information
m2 <- arima(Bev, order = c(1,1,1), xreg = data.frame(Indust))
m2
tsdiag(m2)
```

```{r}
m3 <- arima(Bev, order = c(1,1,1), xreg = data.frame(Food))
m3
tsdiag(m3)
```

both seem to be important, so we add both to the model
```{r}
m4 <- arima(Bev, order = c(1,1,1), xreg = data.frame(Food, Indust))
m4
tsdiag(m4)
```

```{r}
# Prediction
par(mfrow = c(1,1))
f.arima <- forecast(m1, h = 3)
plot(f.arima)
rmse.arima <- sqrt(mean((f.arima$mean - test$BevInd)^2))
rmse.arima
```

```{r}
par(mfrow = c(1,1))
f.arimax <- forecast(m4, h = 3, xreg = data.frame(Food = test$FoodInd, Indust = test$IndustInd))
plot(f.arimax)
rmse.arimax <- sqrt(mean((f.arimax$mean - test$BevInd)^2))
rmse.arimax

```

```{r}
summary(VAR(y = data.frame(Bev, Food, Indust), p = 1))
VAR(y = data.frame(Bev, Food, Indust), p = 2)
VAR(y = data.frame(Bev, Food, Indust), p = 3)

```

```{r}
VARselect(y = data.frame(Bev, Food, Indust))
```

```{r}
m.var <- VAR(y = data.frame(Bev, Food, Indust), p = 2)
plot(m.var)
```

```{r}
pred <- predict(m.var, n.ahead = 3, ci = 0.95)
plot(pred)
rmse.var <- sqrt(mean((pred$fcst$Bev[,1] - test$BevInd)^2))
rmse.var
```

